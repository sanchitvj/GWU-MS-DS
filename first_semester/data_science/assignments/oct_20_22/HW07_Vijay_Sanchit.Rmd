---
title: "Intro to DS - Linear Model Part I"
author: "GWU Intro to Data Science DATS 6101"
# date: "today"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW 7: Linear models - Quantitative Regressors 

This homework uses a bikeshare dataset called `BikeShare` on api.regression.fit, or `bikedata.csv` on our class GitHub repo.  

### Question 1  
**Import the data and name it `bikeorig`.  Remove `Date`, `Casual_Users`, and `Registered_Users` from the dataset and save it as a new data frame named `bike`.  How many variables are in `bike`? How many of them are imported as `int`? Feel free to rename longer variable names into shorter ones for convenience.**
```{r, results='markup'}
bikeorig = data.frame(read.csv("bikedata.csv"))
bike = subset(bikeorig, select = -c(Casual.Users, Registered.Users, Date))
# bike
```
There are `r format(ncol(bike))` variables in 'bike'.  

```{r, results='markup'}
table(unlist(sapply(bike, is.integer)))
```

From above we can see that there are 9 variables with 'int' data type.  

### Question 2    
**Select the subset with `Hour` equal 16 only and name the new data frame `bike16`.  These are the afternoon rush hour data. How many observations are there?**  
```{r, results='markup'}
bike16 = subset(bike, Hour == 16)
# bike16
```
There are `r format(nrow(bike16))` observations in new dataframe.  

### Question 3  
**Before building any models, we should make sure the variables are set up properly.  Which ones should be recorded as categorical? Convert them now before proceeding to the model building.**

We will go ahead and answer this question for you...

```{r, results='markup'}
bike_final = bike16
bike_final$Season = factor(bike16$Season)
bike_final$Holiday = factor(bike16$Holiday)
bike_final$Day = factor(bike16$Day)
bike_final$Workday = factor(bike16$Working.Day)
bike_final$Weather = factor(bike16$Weather)
str(bike_final)
```

The above code converts the following variables into categorical (factor):  
`Season`, `Holiday`, `Day`, `Workday`, and `Weather`.  Notice that 
the data frame `bike16` still has all variables numerical, while the data frame `bike_final` 
has the categorical columns that we just converted. 

### Question 4  
**Make a `pairs()` plot with all the variables (quantitative and qualitative) in the `bike_final` dataset.**  

Note: While the `cor()` function does not accept categorical variables (and therefore 
we cannot use it for `corrplot()`), the `lattice::pairs()` function does not complain 
about categorical columns. We can still use it to get a visual distribution of 
data values from it.
```{r, results='markup'}
pairs(bike_final)
```


### Question 5  
**Make a `corrplot()` with only the numerical variables in the `bike_final` dataset.**  

Note: correlation functions will not work with categorical/factor variables. 
You can either subset the data frame to only numerical variables first, then create 
the correlation matrix to plot. Or you can create the correlation matrix from 
`bike16`, then select out the portion of the matrix that you want. 
Use options that does a good job showing the relationships between different variables. 
```{r, results='markup'}
# bike_cor = cor(bike_final, method = 'pearson')
bike_cor = cor(bike_final[sapply(bike_final,is.numeric)])
corrplot::corrplot(bike_cor, method = 'number')
```


### Question 6   
**Using ony the numerical variables from the `bike_final` dataset, build a linear model with 1 independent variable to predict the `Total Users`.  Choose the variable with the strongest correlation coefficient. Comment on the coefficient values, their p-values, and the multiple R-squared value.**  
```{r, results='markup'}
# colnames(bike_final)[which(colnames(bike_final) == 'Temparature.Feels.F')] <- "temparature_feels_f"
colnames(bike_final)[8] <- "temparature_feels_f"
# bike_final

lm_model = lm(Total.Users ~ temparature_feels_f, data = bike_final)
summary(lm_model)

xkablevif(lm_model)
```
Since the intercept is -0.15, the base model's value is -0.15, and the p-value is less than 0.05, the null hypothesis can be disregarded. The multiple R2 value for the model is 0.309, which is not satisfactory.  


### Question 7   
**Next, add a second variable to the model.  Choose the variable with the next strongest correlation, but avoid using obviously collinear variables (`TempF` and `TempFF` for example).  Comment on the coefficient values, their p-values, and the multiple R-squared value.**

Note: When you have the model, check the VIF values. If the VIF is higher than 5, discard this model, and try the 
variable with the next strongest correlation until you find one that works 
(ideally with VIF<5, or if you have to, allow VIF up to 10).  
```{r, results='markup'}
lm_model_2 = lm(Total.Users ~ temparature_feels_f + Weather.Type, data = bike_final)
summary(lm_model_2)

xkablevif(lm_model_2)
```
The predictor variables temperature and humidity are statistically significant at the 0.05 significance level with an R2 of 0.35 and a p value for both variables less than 0.05. When a vif value is less than 5, it can be used as a predictor.  


### Question 8  
**We will try one more time as in the previous question, to add a third numeric variable in our model.**  
```{r, results='markup'}
lm_model_3 = lm(Total.Users ~ temparature_feels_f + Weather.Type + Humidity, data = bike_final)
summary(lm_model_3)

xkablevif(lm_model_3)
```


### Question 9  
**For the three variable model you found in Q8, find the confidence intervals of the coefficients.**  
```{r, results='markup'}
lm_model_3$coefficients
confint(lm_model_3)
```


### Question 10    
**Use ANOVA to compare the three different models you found. Interpret the results. What conclusion can you draw?**  
```{r, results='markup'}
anova_comp = anova(lm_model, lm_model_2, lm_model_3)
anova_comp
```


Model three is the best model, according to a comparison of all the models, as its RSS value is lower and its p value is less than 0.05.  





